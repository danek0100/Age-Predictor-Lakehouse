FROM python:3.9-slim

# Установка Java
RUN apt-get update && apt-get install -y \
    openjdk-11-jdk \
    && rm -rf /var/lib/apt/lists/*

# Установка Spark
ENV SPARK_VERSION=3.0.0
ENV HADOOP_VERSION=3.2
ENV SPARK_HOME=/opt/spark
ENV PATH=$PATH:$SPARK_HOME/bin

RUN wget https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz \
    && tar xzf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz \
    && mv spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} ${SPARK_HOME} \
    && rm spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz

# Установка Python зависимостей
RUN pip install --no-cache-dir \
    numpy==1.21.1 \
    pandas \
    tqdm \
    psutil \
    matplotlib \
    requests \
    pyspark==3.0.0

# Создание рабочей директории
WORKDIR /opt/spark-apps

# Копирование конфигурации Spark
COPY spark-defaults.conf ${SPARK_HOME}/conf/

# Запуск Spark в локальном режиме
CMD ["spark-submit", "--master", "local[*]", "--deploy-mode", "client", "/opt/spark-apps/regularSpark.py"] 