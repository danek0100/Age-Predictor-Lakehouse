{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# ETL Pipeline с использованием Delta Lake\n",
    "\n",
    "В этом ноутбуке реализован ETL-пайплайн для обработки данных с использованием Delta Lake. Процесс включает следующие этапы:\n",
    "\n",
    "1. **Загрузка исходных данных в формат Delta Table (bronze слой)**\n",
    "2. **Очистка и трансформация данных**\n",
    "3. **Сохранение обработанных данных в silver слой**\n",
    "\n",
    "## Требования к проекту:\n",
    "- Сформировать корректный датасет для предсказания возраста по устройству\n",
    "- Объединить данные из target_train.feather и dataset_full.feather по ID\n",
    "- Удалить дубликаты записей по устройству для одного пользователя\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pyspark==3.5.0 delta-spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Импорт библиотек завершен успешно\n"
     ]
    }
   ],
   "source": [
    "# Импорт необходимых библиотек\n",
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta import *\n",
    "import pyarrow.dataset as ds\n",
    "\n",
    "print(\"Импорт библиотек завершен успешно\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Директории созданы:\n",
      "Bronze: /opt/data/bronze\n",
      "Silver: /opt/data/silver\n"
     ]
    }
   ],
   "source": [
    "# Создание директорий для bronze и silver слоев\n",
    "bronze_path = \"/opt/data/bronze\"\n",
    "silver_path = \"/opt/data/silver\"\n",
    "\n",
    "# Создаем папки, если они не существуют\n",
    "os.makedirs(bronze_path, exist_ok=True)\n",
    "os.makedirs(silver_path, exist_ok=True)\n",
    "\n",
    "print(f\"Директории созданы:\")\n",
    "print(f\"Bronze: {bronze_path}\")\n",
    "print(f\"Silver: {silver_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to: spark://spark-standalone:7077\n",
      "Spark version: 3.5.0\n",
      "Delta Lake support enabled\n",
      "Available cores: 2\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "import os\n",
    "\n",
    "# На всякий случай сбросим старые настройки spark-submit\n",
    "os.environ.pop(\"PYSPARK_SUBMIT_ARGS\", None)\n",
    "\n",
    "# Собираем builder сразу с настройками кластера и Delta Lake\n",
    "builder = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"Age Predictor ETL Pipeline\")\n",
    "        .master(\"spark://spark-standalone:7077\")\n",
    "        .config(\"spark.driver.host\", \"jupyter-spark\")\n",
    "        .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "        .config(\"spark.driver.port\", \"7078\")\n",
    "        .config(\"spark.blockManager.port\", \"7079\")\n",
    "\n",
    "        # Delta Lake extensions\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "        # Общие оптимизации\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    ")\n",
    "\n",
    "# Оборачиваем builder, чтобы подтянулись все JAR’ы Delta\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "print(\"Connected to:\", spark.sparkContext.master)\n",
    "print(\"Spark version:\", spark.version)\n",
    "print(\"Delta Lake support enabled\")\n",
    "print(\"Available cores:\", spark.sparkContext.defaultParallelism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Этап 1: Загрузка данных в Bronze слой\n",
    "\n",
    "Загрузим исходные данные из feather файлов и сохраним их в формате Delta Table в папку bronze.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка target_train.feather...\n",
      "Размер target_train: (270000, 3)\n",
      "Колонки target_train: ['age', 'is_male', 'user_id']\n",
      "\n",
      "Пример данных target_train:\n",
      "    age is_male  user_id\n",
      "0  31.0       1   350459\n",
      "1  35.0       1   188276\n",
      "2  41.0       0    99002\n",
      "3  33.0       0   155506\n",
      "4  54.0       0   213873\n"
     ]
    }
   ],
   "source": [
    "# Загрузка данных из feather файлов\n",
    "print(\"Загрузка target_train.feather...\")\n",
    "target_train_pandas = pd.read_feather(\"../data/target_train.feather\")\n",
    "print(f\"Размер target_train: {target_train_pandas.shape}\")\n",
    "print(f\"Колонки target_train: {list(target_train_pandas.columns)}\")\n",
    "\n",
    "# Просмотр первых строк\n",
    "print(\"\\nПример данных target_train:\")\n",
    "print(target_train_pandas.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Конвертация в Spark DataFrame...\n",
      "Схема target_train:\n",
      "root\n",
      " |-- age: double (nullable = true)\n",
      " |-- is_male: string (nullable = true)\n",
      " |-- user_id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Конвертация pandas DataFrame в Spark DataFrame\n",
    "print(\"Конвертация в Spark DataFrame...\")\n",
    "target_train_spark = spark.createDataFrame(target_train_pandas)\n",
    "\n",
    "print(\"Схема target_train:\")\n",
    "target_train_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Абсолютный путь до целевой папки\n",
    "target_train_path = bronze_path + \"/target_train\"\n",
    "\n",
    "# Пишем в Delta\n",
    "target_train_spark.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(target_train_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Загружаем основной датасет частями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка dataset_full.feather по батчам и сохранение в Bronze слой…\n",
      "Первый батч размером: (65536, 12)\n",
      "Колонки: ['region_name', 'city_name', 'cpe_manufacturer_name', 'cpe_model_name', 'url_host', 'cpe_type_cd', 'cpe_model_os_type', 'price', 'date', 'part_of_day', 'request_cnt', 'user_id']\n",
      "\n",
      "Схема dataset_full:\n",
      "root\n",
      " |-- region_name: string (nullable = true)\n",
      " |-- city_name: string (nullable = true)\n",
      " |-- cpe_manufacturer_name: string (nullable = true)\n",
      " |-- cpe_model_name: string (nullable = true)\n",
      " |-- url_host: string (nullable = true)\n",
      " |-- cpe_type_cd: string (nullable = true)\n",
      " |-- cpe_model_os_type: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- part_of_day: string (nullable = true)\n",
      " |-- request_cnt: long (nullable = true)\n",
      " |-- user_id: long (nullable = true)\n",
      "\n",
      "Батч 1: записано 65536 строк (итого 65536)\n",
      "Батч 26: записано 65536 строк (итого 1703936)\n",
      "Батч 51: записано 65536 строк (итого 3342336)\n",
      "Батч 76: записано 65536 строк (итого 4980736)\n",
      "Батч 101: записано 65536 строк (итого 6619136)\n",
      "Батч 126: записано 65536 строк (итого 8257536)\n",
      "Батч 151: записано 65536 строк (итого 9895936)\n",
      "Батч 176: записано 65536 строк (итого 11534336)\n",
      "Батч 201: записано 65536 строк (итого 13172736)\n",
      "Батч 226: записано 65536 строк (итого 14811136)\n",
      "Батч 251: записано 65536 строк (итого 16449536)\n",
      "Батч 276: записано 65536 строк (итого 18087936)\n",
      "Батч 301: записано 65536 строк (итого 19726336)\n",
      "\n",
      "Всего строк записано: 20054016\n",
      "Данные успешно сохранены в bronze слой!\n"
     ]
    }
   ],
   "source": [
    "target_path = bronze_path + \"/dataset_full\"\n",
    "\n",
    "# Загружаем Feather через PyArrow Dataset и пишем батчами\n",
    "print(\"Загрузка dataset_full.feather по батчам и сохранение в Bronze слой…\")\n",
    "dataset = ds.dataset(\"../data/dataset_full.feather\", format=\"feather\")\n",
    "\n",
    "MAX_ROWS = 20_000_000\n",
    "batch_size = 200_000\n",
    "total_rows = 0\n",
    "\n",
    "for i, batch in enumerate(dataset.to_batches(batch_size=batch_size)):\n",
    "    # если уже накопили нужное — выходим\n",
    "    if total_rows >= MAX_ROWS:\n",
    "        break\n",
    "        \n",
    "    # Конвертация Arrow batch → Pandas\n",
    "    pdf = batch.to_pandas()\n",
    "    if i == 0:\n",
    "        print(f\"Первый батч размером: {pdf.shape}\")\n",
    "        print(f\"Колонки: {list(pdf.columns)}\")\n",
    "\n",
    "    total_rows += len(pdf)\n",
    "\n",
    "    # Конвертация в Spark DataFrame\n",
    "    sdf = spark.createDataFrame(pdf)\n",
    "    if i == 0:\n",
    "        print(\"\\nСхема dataset_full:\")\n",
    "        sdf.printSchema()\n",
    "\n",
    "    # Запись в Delta в режиме append + mergeSchema\n",
    "    sdf.write \\\n",
    "       .format(\"delta\") \\\n",
    "       .mode(\"append\") \\\n",
    "       .option(\"mergeSchema\", \"true\") \\\n",
    "       .save(target_path)\n",
    "\n",
    "    if i % 25 == 0:\n",
    "        print(f\"Батч {i+1}: записано {len(pdf)} строк (итого {total_rows})\")\n",
    "\n",
    "print(f\"\\nВсего строк записано: {total_rows}\")\n",
    "print(\"Данные успешно сохранены в bronze слой!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Этап 2: Загрузка данных из Bronze слоя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка данных из bronze слоя...\n",
      "Количество записей в target_train: 270000\n",
      "Количество записей в dataset_full: 20054016\n",
      "Уникальных ID в target_train: 270000\n",
      "Уникальных ID в dataset_full: 29884\n"
     ]
    }
   ],
   "source": [
    "# Загрузка данных из bronze слоя\n",
    "print(\"Загрузка данных из bronze слоя...\")\n",
    "target_train_bronze = spark.read.format(\"delta\").load(f\"{bronze_path}/target_train\")\n",
    "dataset_full_bronze = spark.read.format(\"delta\").load(f\"{bronze_path}/dataset_full\")\n",
    "\n",
    "print(f\"Количество записей в target_train: {target_train_bronze.count()}\")\n",
    "print(f\"Количество записей в dataset_full: {dataset_full_bronze.count()}\")\n",
    "\n",
    "# Проверка уникальных ID в каждом датасете\n",
    "print(f\"Уникальных ID в target_train: {target_train_bronze.select('user_id').distinct().count()}\")\n",
    "print(f\"Уникальных ID в dataset_full: {dataset_full_bronze.select('user_id').distinct().count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Этап 3: Очистка и трансформация данных\n",
    "\n",
    "Здесь мы выполним следующие операции:\n",
    "1. Объединим данные по ID пользователя\n",
    "2. Удалим дубликаты записей по устройству для одного пользователя\n",
    "3. Подготовим финальный датасет для предсказания возраста по устройству\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Объединение данных по ID пользователя...\n",
      "Количество записей после объединения: 13109396\n",
      "Количество уникальных пользователей: 19482\n",
      "\n",
      "Схема объединенных данных:\n",
      "root\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- region_name: string (nullable = true)\n",
      " |-- city_name: string (nullable = true)\n",
      " |-- cpe_manufacturer_name: string (nullable = true)\n",
      " |-- cpe_model_name: string (nullable = true)\n",
      " |-- url_host: string (nullable = true)\n",
      " |-- cpe_type_cd: string (nullable = true)\n",
      " |-- cpe_model_os_type: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- part_of_day: string (nullable = true)\n",
      " |-- request_cnt: long (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- is_male: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Объединение данных по ID пользователя\n",
    "print(\"Объединение данных по ID пользователя...\")\n",
    "joined_df = dataset_full_bronze.join(target_train_bronze, on=\"user_id\", how=\"inner\")\n",
    "\n",
    "print(f\"Количество записей после объединения: {joined_df.count()}\")\n",
    "print(f\"Количество уникальных пользователей: {joined_df.select('user_id').distinct().count()}\")\n",
    "\n",
    "# Проверим схему объединенных данных\n",
    "print(\"\\nСхема объединенных данных:\")\n",
    "joined_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Анализ дубликатов по устройству для пользователей...\n",
      "Количество пар (пользователь, устройство) с дубликатами: 19231\n",
      "\n",
      "Примеры дубликатов:\n",
      "+-------+----------------+-----+\n",
      "|user_id|  cpe_model_name|count|\n",
      "+-------+----------------+-----+\n",
      "|  63338|       iPhone 11|15022|\n",
      "| 162592|            1904|12286|\n",
      "| 291085|       iPhone 11|11843|\n",
      "| 160011|       iPhone XR|11661|\n",
      "|  16802|Redmi Note 8 Pro|11607|\n",
      "| 407449|       iPhone 11|11321|\n",
      "| 112660|        iPhone 7|10202|\n",
      "| 269822|  iPhone SE 2020| 9827|\n",
      "| 160651|       iPhone XR| 9674|\n",
      "| 217241|      Galaxy A31| 9494|\n",
      "+-------+----------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Анализ дубликатов\n",
    "print(\"Анализ дубликатов по устройству для пользователей...\")\n",
    "\n",
    "# Подсчет записей на пользователя и устройство\n",
    "user_device_counts = joined_df.groupBy(\"user_id\", \"cpe_model_name\").count()\n",
    "duplicates = user_device_counts.filter(col(\"count\") > 1)\n",
    "\n",
    "print(f\"Количество пар (пользователь, устройство) с дубликатами: {duplicates.count()}\")\n",
    "\n",
    "# Показать примеры дубликатов\n",
    "if duplicates.count() > 0:\n",
    "    print(\"\\nПримеры дубликатов:\")\n",
    "    duplicates.orderBy(desc(\"count\")).show(10)\n",
    "else:\n",
    "    print(\"Дубликаты не найдены\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество записей после очистки: 19482\n",
      "Удалено записей: 13089914\n"
     ]
    }
   ],
   "source": [
    "# Удаление дубликатов по устройству для каждого пользователя\n",
    "print(\"Удаление дубликатов...\")\n",
    "\n",
    "# Определяем колонки для удаления дубликатов (id и device)\n",
    "columns_to_check = joined_df.columns\n",
    "print(f\"Доступные колонки: {columns_to_check}\")\n",
    "\n",
    "# Ищем колонки, которые могут содержать информацию об устройстве\n",
    "device_related_columns = [col for col in columns_to_check if 'cpe_model_name' in col.lower() or 'cpe_manufacturer_name' in col.lower()]\n",
    "print(f\"Колонки, связанные с устройством: {device_related_columns}\")\n",
    "\n",
    "# Если есть колонка device, используем её для удаления дубликатов\n",
    "if 'cpe_model_name' in columns_to_check:\n",
    "    cleaned_df = joined_df.dropDuplicates([\"user_id\", \"cpe_model_name\"])\n",
    "    print(\"Используется колонка 'cpe_model_name' для удаления дубликатов\")\n",
    "elif device_related_columns:\n",
    "    # Используем первую найденную колонку, связанную с устройством\n",
    "    device_col = device_related_columns[0]\n",
    "    cleaned_df = joined_df.dropDuplicates([\"user_id\", device_col])\n",
    "    print(f\"Используется колонка '{device_col}' для удаления дубликатов\")\n",
    "else:\n",
    "    # Если колонки device нет, удаляем дубликаты по всем колонкам\n",
    "    cleaned_df = joined_df.dropDuplicates()\n",
    "    print(\"Удаляем полные дубликаты записей\")\n",
    "\n",
    "print(f\"Количество записей до очистки: {joined_df.count()}\")\n",
    "print(f\"Количество записей после очистки: {cleaned_df.count()}\")\n",
    "print(f\"Удалено записей: {joined_df.count() - cleaned_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Финальная статистика очищенных данных:\n",
      "Общее количество записей: 19482\n",
      "Количество уникальных пользователей: 19482\n",
      "\n",
      "Статистика по возрасту:\n",
      "+-------+-----+\n",
      "|summary|  age|\n",
      "+-------+-----+\n",
      "|  count|19482|\n",
      "|   mean|  NaN|\n",
      "| stddev|  NaN|\n",
      "|    min| 14.0|\n",
      "|    max|  NaN|\n",
      "+-------+-----+\n",
      "\n",
      "\n",
      "Пример очищенных данных:\n",
      "+-------+--------------------+--------------------+---------------------+-------------------+---------------------------+-----------+-----------------+-------+-------------------+-----------+-----------+----+-------+\n",
      "|user_id|region_name         |city_name           |cpe_manufacturer_name|cpe_model_name     |url_host                   |cpe_type_cd|cpe_model_os_type|price  |date               |part_of_day|request_cnt|age |is_male|\n",
      "+-------+--------------------+--------------------+---------------------+-------------------+---------------------------+-----------+-----------------+-------+-------------------+-----------+-----------+----+-------+\n",
      "|16     |Челябинская область |Магнитогорск        |Samsung              |Galaxy A10 Dual    |avatars.mds.yandex.net     |smartphone |Android          |9583.0 |2021-06-29 00:00:00|day        |1          |63.0|0      |\n",
      "|26     |Санкт-Петербург     |Санкт-Петербург     |Samsung              |Galaxy A7 2017 Dual|ad.mail.ru                 |smartphone |Android          |4990.0 |2021-06-30 00:00:00|evening    |1          |59.0|0      |\n",
      "|29     |Московская область  |Люберцы             |Apple                |iPhone 8 Plus      |google.com                 |smartphone |iOS              |45696.0|2021-06-29 00:00:00|day        |1          |30.0|0      |\n",
      "|61     |Краснодарский край  |Туапсе              |Apple                |iPhone XR          |apple.com                  |smartphone |iOS              |49572.0|2022-02-27 00:00:00|day        |1          |25.0|0      |\n",
      "|125    |Смоленская область  |Вязьма              |Huawei               |Honor 8A           |careerix.ru                |smartphone |Android          |8010.0 |2021-07-06 00:00:00|morning    |1          |43.0|0      |\n",
      "|130    |Чувашская Республика|Чебоксары           |Huawei               |Honor 8X           |ad.mail.ru                 |smartphone |Android          |18334.0|2021-06-28 00:00:00|morning    |2          |50.0|0      |\n",
      "|153    |Республика Татарстан|Набережные Челны    |Huawei               |Honor 9 Dual       |exchange.buzzoola.com      |smartphone |Android          |3890.0 |2021-06-30 00:00:00|evening    |1          |56.0|1      |\n",
      "|158    |Ростовская область  |Шахты               |Samsung              |Galaxy A52         |googleads.g.doubleclick.net|smartphone |Android          |29772.0|2021-07-16 00:00:00|morning    |3          |35.0|0      |\n",
      "|163    |Оренбургская область|Соль-Илецк          |Huawei               |Honor 10 Lite      |google.com                 |smartphone |Android          |13959.0|2021-06-25 00:00:00|evening    |2          |34.0|0      |\n",
      "|229    |Хабаровский край    |Комсомольск-на-Амуре|Apple                |iPhone 11          |ads.adfox.ru               |smartphone |iOS              |55562.0|2021-08-03 00:00:00|evening    |1          |25.0|0      |\n",
      "+-------+--------------------+--------------------+---------------------+-------------------+---------------------------+-----------+-----------------+-------+-------------------+-----------+-----------+----+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Финальная проверка и статистика данных\n",
    "print(\"Финальная статистика очищенных данных:\")\n",
    "print(f\"Общее количество записей: {cleaned_df.count()}\")\n",
    "print(f\"Количество уникальных пользователей: {cleaned_df.select('user_id').distinct().count()}\")\n",
    "\n",
    "# Статистика по возрасту\n",
    "print(\"\\nСтатистика по возрасту:\")\n",
    "age_stats = cleaned_df.select(\"age\").describe()\n",
    "age_stats.show()\n",
    "\n",
    "# Показать примеры финальных данных\n",
    "print(\"\\nПример очищенных данных:\")\n",
    "cleaned_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Этап 4: Сохранение в Silver слой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сохранение очищенных данных в silver слой...\n",
      "Данные успешно сохранены в silver слой!\n",
      "Метаданные процесса сохранены\n"
     ]
    }
   ],
   "source": [
    "# Сохранение очищенных данных в silver слой\n",
    "print(\"Сохранение очищенных данных в silver слой...\")\n",
    "\n",
    "cleaned_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(f\"{silver_path}/cleaned_dataset\")\n",
    "\n",
    "print(\"Данные успешно сохранены в silver слой!\")\n",
    "\n",
    "# Добавим метаданные о процессе\n",
    "metadata_df = spark.createDataFrame([\n",
    "    (\"bronze_to_silver_etl\", \"completed\", \n",
    "     joined_df.count(), cleaned_df.count(), \n",
    "     joined_df.count() - cleaned_df.count())\n",
    "], [\"process\", \"status\", \"input_records\", \"output_records\", \"removed_duplicates\"])\n",
    "\n",
    "metadata_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"{silver_path}/process_metadata\")\n",
    "\n",
    "print(\"Метаданные процесса сохранены\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Проверка сохраненных данных в silver слое...\n",
      "Количество записей в silver слое: 19482\n",
      "\n",
      "Схема данных в silver слое:\n",
      "root\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- region_name: string (nullable = true)\n",
      " |-- city_name: string (nullable = true)\n",
      " |-- cpe_manufacturer_name: string (nullable = true)\n",
      " |-- cpe_model_name: string (nullable = true)\n",
      " |-- url_host: string (nullable = true)\n",
      " |-- cpe_type_cd: string (nullable = true)\n",
      " |-- cpe_model_os_type: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- part_of_day: string (nullable = true)\n",
      " |-- request_cnt: long (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- is_male: string (nullable = true)\n",
      "\n",
      "\n",
      "Пример данных из silver слоя:\n",
      "+-------+-------------------+---------------+---------------------+-------------------+----------------------+-----------+-----------------+-------+-------------------+-----------+-----------+----+-------+\n",
      "|user_id|region_name        |city_name      |cpe_manufacturer_name|cpe_model_name     |url_host              |cpe_type_cd|cpe_model_os_type|price  |date               |part_of_day|request_cnt|age |is_male|\n",
      "+-------+-------------------+---------------+---------------------+-------------------+----------------------+-----------+-----------------+-------+-------------------+-----------+-----------+----+-------+\n",
      "|16     |Челябинская область|Магнитогорск   |Samsung              |Galaxy A10 Dual    |avatars.mds.yandex.net|smartphone |Android          |9583.0 |2021-06-29 00:00:00|day        |1          |63.0|0      |\n",
      "|26     |Санкт-Петербург    |Санкт-Петербург|Samsung              |Galaxy A7 2017 Dual|ad.mail.ru            |smartphone |Android          |4990.0 |2021-06-30 00:00:00|evening    |1          |59.0|0      |\n",
      "|29     |Московская область |Люберцы        |Apple                |iPhone 8 Plus      |google.com            |smartphone |iOS              |45696.0|2021-06-29 00:00:00|day        |1          |30.0|0      |\n",
      "|61     |Краснодарский край |Туапсе         |Apple                |iPhone XR          |apple.com             |smartphone |iOS              |49572.0|2022-02-27 00:00:00|day        |1          |25.0|0      |\n",
      "|125    |Смоленская область |Вязьма         |Huawei               |Honor 8A           |careerix.ru           |smartphone |Android          |8010.0 |2021-07-06 00:00:00|morning    |1          |43.0|0      |\n",
      "+-------+-------------------+---------------+---------------------+-------------------+----------------------+-----------+-----------------+-------+-------------------+-----------+-----------+----+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Метаданные процесса:\n",
      "+--------------------+---------+-------------+--------------+------------------+\n",
      "|             process|   status|input_records|output_records|removed_duplicates|\n",
      "+--------------------+---------+-------------+--------------+------------------+\n",
      "|bronze_to_silver_etl|completed|     13109396|         19482|          13089914|\n",
      "+--------------------+---------+-------------+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Проверка сохраненных данных в silver слое\n",
    "print(\"Проверка сохраненных данных в silver слое...\")\n",
    "\n",
    "silver_data = spark.read.format(\"delta\").load(f\"{silver_path}/cleaned_dataset\")\n",
    "print(f\"Количество записей в silver слое: {silver_data.count()}\")\n",
    "\n",
    "print(\"\\nСхема данных в silver слое:\")\n",
    "silver_data.printSchema()\n",
    "\n",
    "print(\"\\nПример данных из silver слоя:\")\n",
    "silver_data.show(5, truncate=False)\n",
    "\n",
    "# Проверка метаданных\n",
    "metadata = spark.read.format(\"delta\").load(f\"{silver_path}/process_metadata\")\n",
    "print(\"\\nМетаданные процесса:\")\n",
    "metadata.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Этап 5: Финальная валидация и сводка\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== СВОДКА ETL ПРОЦЕССА ===\n",
      "\n",
      "СТАТИСТИКА ДАННЫХ:\n",
      "Bronze - target_train: 270,000 записей\n",
      "Bronze - dataset_full: 20,054,016 записей\n",
      "Silver - cleaned_dataset: 19,482 записей\n",
      "\n",
      "ВЫПОЛНЕННЫЕ ЗАДАЧИ:\n",
      "1. Загружены исходные данные в формат Delta Table в папку ./data/bronze/\n",
      "2. Данные объединены по ID пользователя\n",
      "3. Удалены дубликаты записей по устройству для одного пользователя\n",
      "4. Обработанные данные сохранены в папку silver в формате Delta Table\n",
      "\n",
      "СТРУКТУРА ДАННЫХ:\n",
      "../data/bronze/target_train/ - исходные данные о возрасте пользователей\n",
      "../data/bronze/dataset_full/ - исходные данные об устройствах пользователей\n",
      "../data/silver/cleaned_dataset/ - очищенный датасет для ML модели\n",
      "../data/silver/process_metadata/ - метаданные ETL процесса\n",
      "\n",
      "Данные готовы для предсказания возраста пользователей по их устройствам!\n"
     ]
    }
   ],
   "source": [
    "# Финальная валидация ETL процесса\n",
    "print(\"=== СВОДКА ETL ПРОЦЕССА ===\")\n",
    "print()\n",
    "\n",
    "# Проверяем наличие данных в bronze и silver слоях\n",
    "bronze_target = spark.read.format(\"delta\").load(f\"{bronze_path}/target_train\")\n",
    "bronze_dataset = spark.read.format(\"delta\").load(f\"{bronze_path}/dataset_full\")\n",
    "silver_cleaned = spark.read.format(\"delta\").load(f\"{silver_path}/cleaned_dataset\")\n",
    "\n",
    "print(\"СТАТИСТИКА ДАННЫХ:\")\n",
    "print(f\"Bronze - target_train: {bronze_target.count():,} записей\")\n",
    "print(f\"Bronze - dataset_full: {bronze_dataset.count():,} записей\")\n",
    "print(f\"Silver - cleaned_dataset: {silver_cleaned.count():,} записей\")\n",
    "print()\n",
    "\n",
    "print(\"ВЫПОЛНЕННЫЕ ЗАДАЧИ:\")\n",
    "print(\"1. Загружены исходные данные в формат Delta Table в папку ./data/bronze/\")\n",
    "print(\"2. Данные объединены по ID пользователя\")\n",
    "print(\"3. Удалены дубликаты записей по устройству для одного пользователя\")\n",
    "print(\"4. Обработанные данные сохранены в папку silver в формате Delta Table\")\n",
    "print()\n",
    "\n",
    "print(\"СТРУКТУРА ДАННЫХ:\")\n",
    "print(\"../data/bronze/target_train/ - исходные данные о возрасте пользователей\")\n",
    "print(\"../data/bronze/dataset_full/ - исходные данные об устройствах пользователей\") \n",
    "print(\"../data/silver/cleaned_dataset/ - очищенный датасет для ML модели\")\n",
    "print(\"../data/silver/process_metadata/ - метаданные ETL процесса\")\n",
    "print()\n",
    "\n",
    "print(\"Данные готовы для предсказания возраста пользователей по их устройствам!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Остановка Spark сессии...\n",
      "Spark сессия завершена\n"
     ]
    }
   ],
   "source": [
    "# Остановка Spark сессии\n",
    "print(\"Остановка Spark сессии...\")\n",
    "spark.stop()\n",
    "print(\"Spark сессия завершена\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
